{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LGBM Regression on TfIDF of text features and One-Hot-Encoded Categoricals\n",
    "Featues based on Alexandu Papiu's (https://www.kaggle.com/apapiu) script: https://www.kaggle.com/apapiu/ridge-script\n",
    "LGBM based on InfiniteWing's (https://www.kaggle.com/infinitewing) script: https://www.kaggle.com/infinitewing/lightgbm-example\n",
    "\"\"\"\n",
    "#TODO don't use dummies, but categorical features of LightGBM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import gc\n",
    "\n",
    "start = fixstart = time.time()\n",
    "\n",
    "NUM_BRANDS = 2500\n",
    "NAME_MIN_DF = 10\n",
    "MAX_FEAT_DESCP = 50000\n",
    "\n",
    "print(\"Reading in Data\")\n",
    "\n",
    "df_train = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('../input/test.tsv', sep='\\t')\n",
    "\n",
    "df = pd.concat([df_train, df_test], 0)\n",
    "nrow_train = df_train.shape[0]\n",
    "y_train = np.log1p(df_train[\"price\"])\n",
    "\n",
    "del df_train\n",
    "gc.collect()\n",
    "\n",
    "print(df.memory_usage(deep = True))\n",
    "\n",
    "df[\"category_name\"] = df[\"category_name\"].fillna(\"Other\").astype(\"category\")\n",
    "df[\"brand_name\"] = df[\"brand_name\"].fillna(\"unknown\")\n",
    "\n",
    "pop_brands = df[\"brand_name\"].value_counts().index[:NUM_BRANDS]\n",
    "df.loc[~df[\"brand_name\"].isin(pop_brands), \"brand_name\"] = \"Other\"\n",
    "\n",
    "df[\"item_description\"] = df[\"item_description\"].fillna(\"None\")\n",
    "df[\"item_condition_id\"] = df[\"item_condition_id\"].astype(\"category\")\n",
    "df[\"brand_name\"] = df[\"brand_name\"].astype(\"category\")\n",
    "\n",
    "# description contains interesting words\n",
    "interesting_words = ['new', 'perfect', 'fit', 'used', 'super', 'cute', 'excellent',\n",
    "                     'great', 'retail', '[rm]', 'never' ]\n",
    "for word in interesting_words:\n",
    "    df[word] = df['item_description'].apply(lambda x : word in x.lower())\n",
    "\n",
    "\n",
    "print(df.memory_usage(deep = True))\n",
    "\n",
    "print(\"Encodings\")\n",
    "count = CountVectorizer(min_df=NAME_MIN_DF)\n",
    "X_name = count.fit_transform(df[\"name\"])\n",
    "\n",
    "print(\"Category Encoders\")\n",
    "unique_categories = pd.Series(\"/\".join(df[\"category_name\"].unique().astype(\"str\")).split(\"/\")).unique()\n",
    "count_category = CountVectorizer()\n",
    "X_category = count_category.fit_transform(df[\"category_name\"])\n",
    "\n",
    "\n",
    "stopw = stopwords.words('english') + interesting_words + ['cant', 'ask', 'size']\n",
    "print(\"Descp encoders\")\n",
    "count_descp = TfidfVectorizer(max_features = MAX_FEAT_DESCP, \n",
    "                              ngram_range = (1,3),\n",
    "                              stop_words = stopw)\n",
    "X_descp = count_descp.fit_transform(df[\"item_description\"])\n",
    "\n",
    "print(\"Brand encoders\")\n",
    "vect_brand = LabelBinarizer(sparse_output=True)\n",
    "X_brand = vect_brand.fit_transform(df[\"brand_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Encoders\n"
     ]
    }
   ],
   "source": [
    "df[\"new\"] = df[\"new\"].astype(\"category\")\n",
    "print(\"Dummy Encoders\")\n",
    "X_dummies = scipy.sparse.csr_matrix(pd.get_dummies(df[[\n",
    "    'item_condition_id', 'shipping','new']], sparse = True).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2175894, 8), (2175894, 1021), (2175894, 21257), (2175894, 50000), (2175894, 2501)]\n",
      "Time taken reading and encoding  2091.373414993286.\n"
     ]
    }
   ],
   "source": [
    "X = scipy.sparse.hstack((X_dummies, \n",
    "                         X_descp,\n",
    "                         X_brand,\n",
    "                         X_category,\n",
    "                         X_name)).tocsr()\n",
    "\n",
    "print([X_dummies.shape, X_category.shape, \n",
    "       X_name.shape, X_descp.shape, X_brand.shape])\n",
    "\n",
    "X_train = X[:nrow_train]\n",
    "X_test = X[nrow_train:]\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken reading and encoding  {}.\".format((end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482535, 74787)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain's rmse: 0.691604\tvalid's rmse: 0.693228\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\ttrain's rmse: 0.668192\tvalid's rmse: 0.670016\n",
      "[3]\ttrain's rmse: 0.65493\tvalid's rmse: 0.657355\n",
      "[4]\ttrain's rmse: 0.644936\tvalid's rmse: 0.64692\n",
      "[5]\ttrain's rmse: 0.636154\tvalid's rmse: 0.637992\n",
      "[6]\ttrain's rmse: 0.630309\tvalid's rmse: 0.632032\n",
      "[7]\ttrain's rmse: 0.62539\tvalid's rmse: 0.627467\n",
      "[8]\ttrain's rmse: 0.621148\tvalid's rmse: 0.623278\n",
      "[9]\ttrain's rmse: 0.617552\tvalid's rmse: 0.619539\n",
      "[10]\ttrain's rmse: 0.613801\tvalid's rmse: 0.616048\n",
      "[11]\ttrain's rmse: 0.610565\tvalid's rmse: 0.612875\n",
      "[12]\ttrain's rmse: 0.607762\tvalid's rmse: 0.610223\n",
      "[13]\ttrain's rmse: 0.604363\tvalid's rmse: 0.60697\n",
      "[14]\ttrain's rmse: 0.601225\tvalid's rmse: 0.603833\n",
      "[15]\ttrain's rmse: 0.59817\tvalid's rmse: 0.600838\n",
      "[16]\ttrain's rmse: 0.595949\tvalid's rmse: 0.598874\n",
      "[17]\ttrain's rmse: 0.593455\tvalid's rmse: 0.596495\n",
      "[18]\ttrain's rmse: 0.591314\tvalid's rmse: 0.594267\n",
      "[19]\ttrain's rmse: 0.589229\tvalid's rmse: 0.592053\n",
      "[20]\ttrain's rmse: 0.587377\tvalid's rmse: 0.59033\n",
      "[21]\ttrain's rmse: 0.585796\tvalid's rmse: 0.588823\n",
      "[22]\ttrain's rmse: 0.583902\tvalid's rmse: 0.586735\n",
      "[23]\ttrain's rmse: 0.582317\tvalid's rmse: 0.585223\n",
      "[24]\ttrain's rmse: 0.580927\tvalid's rmse: 0.583867\n",
      "[25]\ttrain's rmse: 0.579387\tvalid's rmse: 0.582359\n",
      "[26]\ttrain's rmse: 0.578235\tvalid's rmse: 0.581078\n",
      "[27]\ttrain's rmse: 0.576725\tvalid's rmse: 0.579634\n",
      "[28]\ttrain's rmse: 0.573535\tvalid's rmse: 0.576459\n",
      "[29]\ttrain's rmse: 0.572268\tvalid's rmse: 0.575166\n",
      "[30]\ttrain's rmse: 0.571047\tvalid's rmse: 0.574164\n",
      "[31]\ttrain's rmse: 0.569781\tvalid's rmse: 0.572928\n",
      "[32]\ttrain's rmse: 0.568561\tvalid's rmse: 0.571611\n",
      "[33]\ttrain's rmse: 0.56754\tvalid's rmse: 0.570558\n",
      "[34]\ttrain's rmse: 0.566486\tvalid's rmse: 0.569718\n",
      "[35]\ttrain's rmse: 0.565287\tvalid's rmse: 0.568534\n",
      "[36]\ttrain's rmse: 0.56446\tvalid's rmse: 0.567759\n",
      "[37]\ttrain's rmse: 0.563245\tvalid's rmse: 0.566607\n",
      "[38]\ttrain's rmse: 0.561983\tvalid's rmse: 0.565468\n",
      "[39]\ttrain's rmse: 0.561109\tvalid's rmse: 0.564709\n",
      "[40]\ttrain's rmse: 0.560106\tvalid's rmse: 0.563641\n",
      "[41]\ttrain's rmse: 0.559254\tvalid's rmse: 0.562803\n",
      "[42]\ttrain's rmse: 0.558507\tvalid's rmse: 0.562152\n",
      "[43]\ttrain's rmse: 0.556931\tvalid's rmse: 0.560607\n",
      "[44]\ttrain's rmse: 0.556216\tvalid's rmse: 0.559958\n",
      "[45]\ttrain's rmse: 0.555575\tvalid's rmse: 0.55929\n",
      "[46]\ttrain's rmse: 0.554805\tvalid's rmse: 0.558644\n",
      "[47]\ttrain's rmse: 0.554086\tvalid's rmse: 0.557862\n",
      "[48]\ttrain's rmse: 0.553292\tvalid's rmse: 0.557177\n",
      "[49]\ttrain's rmse: 0.552488\tvalid's rmse: 0.556411\n",
      "[50]\ttrain's rmse: 0.551874\tvalid's rmse: 0.555716\n",
      "[51]\ttrain's rmse: 0.551293\tvalid's rmse: 0.555152\n",
      "[52]\ttrain's rmse: 0.55075\tvalid's rmse: 0.554675\n",
      "[53]\ttrain's rmse: 0.550153\tvalid's rmse: 0.554069\n",
      "[54]\ttrain's rmse: 0.549505\tvalid's rmse: 0.55339\n",
      "[55]\ttrain's rmse: 0.548777\tvalid's rmse: 0.552734\n",
      "[56]\ttrain's rmse: 0.548197\tvalid's rmse: 0.55214\n",
      "[57]\ttrain's rmse: 0.547555\tvalid's rmse: 0.551642\n",
      "[58]\ttrain's rmse: 0.546955\tvalid's rmse: 0.551049\n",
      "[59]\ttrain's rmse: 0.546367\tvalid's rmse: 0.55049\n",
      "[60]\ttrain's rmse: 0.545894\tvalid's rmse: 0.550127\n",
      "[61]\ttrain's rmse: 0.54541\tvalid's rmse: 0.549568\n",
      "[62]\ttrain's rmse: 0.544905\tvalid's rmse: 0.549081\n",
      "[63]\ttrain's rmse: 0.544394\tvalid's rmse: 0.548575\n",
      "[64]\ttrain's rmse: 0.543562\tvalid's rmse: 0.547829\n",
      "[65]\ttrain's rmse: 0.543134\tvalid's rmse: 0.547477\n",
      "[66]\ttrain's rmse: 0.541798\tvalid's rmse: 0.546232\n",
      "[67]\ttrain's rmse: 0.54121\tvalid's rmse: 0.545588\n",
      "[68]\ttrain's rmse: 0.540767\tvalid's rmse: 0.545146\n",
      "[69]\ttrain's rmse: 0.540263\tvalid's rmse: 0.544584\n",
      "[70]\ttrain's rmse: 0.539841\tvalid's rmse: 0.544205\n",
      "[71]\ttrain's rmse: 0.539397\tvalid's rmse: 0.543712\n",
      "[72]\ttrain's rmse: 0.538998\tvalid's rmse: 0.543376\n",
      "[73]\ttrain's rmse: 0.53856\tvalid's rmse: 0.543016\n",
      "[74]\ttrain's rmse: 0.538175\tvalid's rmse: 0.542721\n",
      "[75]\ttrain's rmse: 0.537209\tvalid's rmse: 0.541869\n",
      "[76]\ttrain's rmse: 0.536712\tvalid's rmse: 0.541296\n",
      "[77]\ttrain's rmse: 0.536275\tvalid's rmse: 0.540897\n",
      "[78]\ttrain's rmse: 0.535886\tvalid's rmse: 0.540477\n",
      "[79]\ttrain's rmse: 0.535373\tvalid's rmse: 0.540027\n",
      "[80]\ttrain's rmse: 0.534878\tvalid's rmse: 0.539525\n",
      "[81]\ttrain's rmse: 0.534393\tvalid's rmse: 0.539072\n",
      "[82]\ttrain's rmse: 0.533986\tvalid's rmse: 0.538689\n",
      "[83]\ttrain's rmse: 0.533626\tvalid's rmse: 0.538429\n",
      "[84]\ttrain's rmse: 0.533231\tvalid's rmse: 0.538083\n",
      "[85]\ttrain's rmse: 0.532626\tvalid's rmse: 0.537516\n",
      "[86]\ttrain's rmse: 0.532233\tvalid's rmse: 0.537252\n",
      "[87]\ttrain's rmse: 0.531828\tvalid's rmse: 0.536941\n",
      "[88]\ttrain's rmse: 0.531328\tvalid's rmse: 0.536566\n",
      "[89]\ttrain's rmse: 0.530993\tvalid's rmse: 0.536214\n",
      "[90]\ttrain's rmse: 0.530606\tvalid's rmse: 0.535761\n",
      "[91]\ttrain's rmse: 0.530215\tvalid's rmse: 0.535319\n",
      "[92]\ttrain's rmse: 0.529819\tvalid's rmse: 0.534916\n",
      "[93]\ttrain's rmse: 0.529291\tvalid's rmse: 0.534479\n",
      "[94]\ttrain's rmse: 0.528967\tvalid's rmse: 0.534071\n",
      "[95]\ttrain's rmse: 0.528661\tvalid's rmse: 0.533824\n",
      "[96]\ttrain's rmse: 0.52834\tvalid's rmse: 0.533568\n",
      "[97]\ttrain's rmse: 0.528044\tvalid's rmse: 0.533299\n",
      "[98]\ttrain's rmse: 0.527681\tvalid's rmse: 0.532865\n",
      "[99]\ttrain's rmse: 0.52741\tvalid's rmse: 0.532651\n",
      "[100]\ttrain's rmse: 0.526876\tvalid's rmse: 0.532171\n",
      "[101]\ttrain's rmse: 0.526506\tvalid's rmse: 0.531857\n",
      "[102]\ttrain's rmse: 0.526215\tvalid's rmse: 0.531544\n",
      "[103]\ttrain's rmse: 0.525724\tvalid's rmse: 0.531038\n",
      "[104]\ttrain's rmse: 0.525422\tvalid's rmse: 0.530758\n",
      "[105]\ttrain's rmse: 0.525087\tvalid's rmse: 0.530382\n",
      "[106]\ttrain's rmse: 0.524834\tvalid's rmse: 0.530073\n",
      "[107]\ttrain's rmse: 0.524574\tvalid's rmse: 0.529925\n",
      "[108]\ttrain's rmse: 0.524304\tvalid's rmse: 0.52973\n",
      "[109]\ttrain's rmse: 0.524062\tvalid's rmse: 0.529518\n",
      "[110]\ttrain's rmse: 0.521489\tvalid's rmse: 0.526777\n",
      "[111]\ttrain's rmse: 0.5211\tvalid's rmse: 0.526407\n",
      "[112]\ttrain's rmse: 0.520791\tvalid's rmse: 0.526108\n",
      "[113]\ttrain's rmse: 0.520515\tvalid's rmse: 0.525868\n",
      "[114]\ttrain's rmse: 0.520278\tvalid's rmse: 0.525767\n",
      "[115]\ttrain's rmse: 0.519985\tvalid's rmse: 0.525523\n",
      "[116]\ttrain's rmse: 0.519751\tvalid's rmse: 0.525256\n",
      "[117]\ttrain's rmse: 0.519476\tvalid's rmse: 0.525082\n",
      "[118]\ttrain's rmse: 0.519275\tvalid's rmse: 0.524936\n",
      "[119]\ttrain's rmse: 0.519046\tvalid's rmse: 0.524723\n",
      "[120]\ttrain's rmse: 0.518772\tvalid's rmse: 0.524492\n",
      "[121]\ttrain's rmse: 0.518507\tvalid's rmse: 0.524129\n",
      "[122]\ttrain's rmse: 0.518291\tvalid's rmse: 0.523989\n",
      "[123]\ttrain's rmse: 0.517892\tvalid's rmse: 0.523839\n",
      "[124]\ttrain's rmse: 0.517692\tvalid's rmse: 0.523681\n",
      "[125]\ttrain's rmse: 0.517453\tvalid's rmse: 0.523464\n",
      "[126]\ttrain's rmse: 0.517202\tvalid's rmse: 0.523273\n",
      "[127]\ttrain's rmse: 0.51689\tvalid's rmse: 0.523011\n",
      "[128]\ttrain's rmse: 0.516704\tvalid's rmse: 0.522862\n",
      "[129]\ttrain's rmse: 0.516505\tvalid's rmse: 0.522608\n",
      "[130]\ttrain's rmse: 0.516298\tvalid's rmse: 0.522342\n",
      "[131]\ttrain's rmse: 0.516079\tvalid's rmse: 0.522076\n",
      "[132]\ttrain's rmse: 0.515684\tvalid's rmse: 0.521701\n",
      "[133]\ttrain's rmse: 0.515274\tvalid's rmse: 0.521268\n",
      "[134]\ttrain's rmse: 0.515049\tvalid's rmse: 0.521016\n",
      "[135]\ttrain's rmse: 0.514851\tvalid's rmse: 0.520946\n",
      "[136]\ttrain's rmse: 0.514631\tvalid's rmse: 0.52074\n",
      "[137]\ttrain's rmse: 0.514436\tvalid's rmse: 0.520579\n",
      "[138]\ttrain's rmse: 0.514263\tvalid's rmse: 0.520455\n",
      "[139]\ttrain's rmse: 0.513972\tvalid's rmse: 0.520211\n",
      "[140]\ttrain's rmse: 0.51375\tvalid's rmse: 0.520001\n",
      "[141]\ttrain's rmse: 0.513284\tvalid's rmse: 0.51966\n",
      "[142]\ttrain's rmse: 0.512516\tvalid's rmse: 0.518905\n",
      "[143]\ttrain's rmse: 0.512277\tvalid's rmse: 0.518694\n",
      "[144]\ttrain's rmse: 0.512076\tvalid's rmse: 0.518502\n",
      "[145]\ttrain's rmse: 0.511827\tvalid's rmse: 0.518243\n",
      "[146]\ttrain's rmse: 0.511661\tvalid's rmse: 0.518092\n",
      "[147]\ttrain's rmse: 0.511461\tvalid's rmse: 0.518\n",
      "[148]\ttrain's rmse: 0.511097\tvalid's rmse: 0.517675\n",
      "[149]\ttrain's rmse: 0.510768\tvalid's rmse: 0.517373\n",
      "[150]\ttrain's rmse: 0.510555\tvalid's rmse: 0.517172\n",
      "[151]\ttrain's rmse: 0.510287\tvalid's rmse: 0.516942\n",
      "[152]\ttrain's rmse: 0.510104\tvalid's rmse: 0.516777\n",
      "[153]\ttrain's rmse: 0.50993\tvalid's rmse: 0.516607\n",
      "[154]\ttrain's rmse: 0.509744\tvalid's rmse: 0.516407\n",
      "[155]\ttrain's rmse: 0.509574\tvalid's rmse: 0.516278\n",
      "[156]\ttrain's rmse: 0.509373\tvalid's rmse: 0.516162\n",
      "[157]\ttrain's rmse: 0.509158\tvalid's rmse: 0.515953\n",
      "[158]\ttrain's rmse: 0.508989\tvalid's rmse: 0.515699\n",
      "[159]\ttrain's rmse: 0.508773\tvalid's rmse: 0.515469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[160]\ttrain's rmse: 0.508583\tvalid's rmse: 0.515298\n",
      "[161]\ttrain's rmse: 0.508382\tvalid's rmse: 0.515124\n",
      "[162]\ttrain's rmse: 0.508196\tvalid's rmse: 0.514991\n",
      "[163]\ttrain's rmse: 0.507988\tvalid's rmse: 0.514853\n",
      "[164]\ttrain's rmse: 0.507771\tvalid's rmse: 0.514671\n",
      "[165]\ttrain's rmse: 0.507586\tvalid's rmse: 0.514456\n",
      "[166]\ttrain's rmse: 0.507451\tvalid's rmse: 0.514265\n",
      "[167]\ttrain's rmse: 0.507274\tvalid's rmse: 0.514134\n",
      "[168]\ttrain's rmse: 0.507128\tvalid's rmse: 0.514032\n",
      "[169]\ttrain's rmse: 0.506971\tvalid's rmse: 0.513906\n",
      "[170]\ttrain's rmse: 0.506801\tvalid's rmse: 0.513797\n",
      "[171]\ttrain's rmse: 0.506401\tvalid's rmse: 0.51342\n",
      "[172]\ttrain's rmse: 0.506238\tvalid's rmse: 0.513331\n",
      "[173]\ttrain's rmse: 0.506077\tvalid's rmse: 0.513138\n",
      "[174]\ttrain's rmse: 0.505859\tvalid's rmse: 0.512949\n",
      "[175]\ttrain's rmse: 0.50568\tvalid's rmse: 0.512783\n",
      "[176]\ttrain's rmse: 0.505535\tvalid's rmse: 0.512646\n",
      "[177]\ttrain's rmse: 0.505397\tvalid's rmse: 0.512581\n",
      "[178]\ttrain's rmse: 0.505147\tvalid's rmse: 0.512315\n",
      "[179]\ttrain's rmse: 0.504962\tvalid's rmse: 0.512176\n",
      "[180]\ttrain's rmse: 0.504832\tvalid's rmse: 0.512069\n",
      "[181]\ttrain's rmse: 0.504679\tvalid's rmse: 0.511915\n",
      "[182]\ttrain's rmse: 0.504305\tvalid's rmse: 0.511535\n",
      "[183]\ttrain's rmse: 0.504104\tvalid's rmse: 0.511316\n",
      "[184]\ttrain's rmse: 0.503839\tvalid's rmse: 0.511161\n",
      "[185]\ttrain's rmse: 0.503477\tvalid's rmse: 0.510823\n",
      "[186]\ttrain's rmse: 0.503307\tvalid's rmse: 0.510661\n",
      "[187]\ttrain's rmse: 0.503086\tvalid's rmse: 0.510456\n",
      "[188]\ttrain's rmse: 0.502902\tvalid's rmse: 0.51034\n",
      "[189]\ttrain's rmse: 0.502774\tvalid's rmse: 0.510247\n",
      "[190]\ttrain's rmse: 0.502638\tvalid's rmse: 0.510181\n",
      "[191]\ttrain's rmse: 0.502514\tvalid's rmse: 0.510129\n",
      "[192]\ttrain's rmse: 0.502385\tvalid's rmse: 0.509985\n",
      "[193]\ttrain's rmse: 0.502139\tvalid's rmse: 0.509764\n",
      "[194]\ttrain's rmse: 0.502027\tvalid's rmse: 0.509651\n",
      "[195]\ttrain's rmse: 0.501877\tvalid's rmse: 0.509529\n",
      "[196]\ttrain's rmse: 0.501751\tvalid's rmse: 0.509414\n",
      "[197]\ttrain's rmse: 0.501621\tvalid's rmse: 0.509341\n",
      "[198]\ttrain's rmse: 0.501461\tvalid's rmse: 0.509181\n",
      "[199]\ttrain's rmse: 0.501308\tvalid's rmse: 0.509076\n",
      "[200]\ttrain's rmse: 0.501147\tvalid's rmse: 0.50893\n",
      "[201]\ttrain's rmse: 0.500967\tvalid's rmse: 0.508755\n",
      "[202]\ttrain's rmse: 0.50081\tvalid's rmse: 0.508608\n",
      "[203]\ttrain's rmse: 0.500664\tvalid's rmse: 0.508525\n",
      "[204]\ttrain's rmse: 0.500515\tvalid's rmse: 0.508413\n",
      "[205]\ttrain's rmse: 0.500367\tvalid's rmse: 0.508272\n",
      "[206]\ttrain's rmse: 0.500234\tvalid's rmse: 0.508114\n",
      "[207]\ttrain's rmse: 0.500094\tvalid's rmse: 0.508017\n",
      "[208]\ttrain's rmse: 0.4999\tvalid's rmse: 0.507798\n",
      "[209]\ttrain's rmse: 0.499789\tvalid's rmse: 0.507738\n",
      "[210]\ttrain's rmse: 0.499649\tvalid's rmse: 0.507566\n",
      "[211]\ttrain's rmse: 0.499512\tvalid's rmse: 0.507508\n",
      "[212]\ttrain's rmse: 0.499368\tvalid's rmse: 0.507415\n",
      "[213]\ttrain's rmse: 0.499235\tvalid's rmse: 0.507306\n",
      "[214]\ttrain's rmse: 0.499115\tvalid's rmse: 0.507195\n",
      "[215]\ttrain's rmse: 0.498946\tvalid's rmse: 0.506985\n",
      "[216]\ttrain's rmse: 0.49871\tvalid's rmse: 0.506859\n",
      "[217]\ttrain's rmse: 0.498602\tvalid's rmse: 0.506778\n",
      "[218]\ttrain's rmse: 0.498464\tvalid's rmse: 0.506712\n",
      "[219]\ttrain's rmse: 0.498288\tvalid's rmse: 0.506588\n",
      "[220]\ttrain's rmse: 0.498183\tvalid's rmse: 0.506487\n",
      "[221]\ttrain's rmse: 0.498064\tvalid's rmse: 0.506388\n",
      "[222]\ttrain's rmse: 0.497943\tvalid's rmse: 0.506277\n",
      "[223]\ttrain's rmse: 0.497843\tvalid's rmse: 0.506221\n",
      "[224]\ttrain's rmse: 0.497323\tvalid's rmse: 0.505765\n",
      "[225]\ttrain's rmse: 0.497154\tvalid's rmse: 0.505624\n",
      "[226]\ttrain's rmse: 0.497042\tvalid's rmse: 0.50547\n",
      "[227]\ttrain's rmse: 0.496837\tvalid's rmse: 0.505295\n",
      "[228]\ttrain's rmse: 0.496702\tvalid's rmse: 0.505193\n",
      "[229]\ttrain's rmse: 0.496492\tvalid's rmse: 0.504991\n",
      "[230]\ttrain's rmse: 0.496361\tvalid's rmse: 0.504848\n",
      "[231]\ttrain's rmse: 0.496266\tvalid's rmse: 0.504791\n",
      "[232]\ttrain's rmse: 0.495977\tvalid's rmse: 0.504549\n",
      "[233]\ttrain's rmse: 0.495851\tvalid's rmse: 0.504422\n",
      "[234]\ttrain's rmse: 0.495759\tvalid's rmse: 0.504347\n",
      "[235]\ttrain's rmse: 0.495595\tvalid's rmse: 0.504166\n",
      "[236]\ttrain's rmse: 0.49548\tvalid's rmse: 0.504058\n",
      "[237]\ttrain's rmse: 0.495338\tvalid's rmse: 0.50394\n",
      "[238]\ttrain's rmse: 0.495214\tvalid's rmse: 0.503834\n",
      "[239]\ttrain's rmse: 0.495065\tvalid's rmse: 0.503733\n",
      "[240]\ttrain's rmse: 0.494955\tvalid's rmse: 0.503613\n",
      "[241]\ttrain's rmse: 0.494671\tvalid's rmse: 0.503281\n",
      "[242]\ttrain's rmse: 0.494526\tvalid's rmse: 0.503134\n",
      "[243]\ttrain's rmse: 0.494367\tvalid's rmse: 0.503076\n",
      "[244]\ttrain's rmse: 0.494205\tvalid's rmse: 0.502923\n",
      "[245]\ttrain's rmse: 0.49406\tvalid's rmse: 0.502762\n",
      "[246]\ttrain's rmse: 0.493746\tvalid's rmse: 0.502459\n",
      "[247]\ttrain's rmse: 0.493611\tvalid's rmse: 0.502327\n",
      "[248]\ttrain's rmse: 0.493477\tvalid's rmse: 0.502233\n",
      "[249]\ttrain's rmse: 0.493335\tvalid's rmse: 0.502103\n",
      "[250]\ttrain's rmse: 0.493222\tvalid's rmse: 0.502004\n",
      "[251]\ttrain's rmse: 0.493099\tvalid's rmse: 0.501822\n",
      "[252]\ttrain's rmse: 0.492978\tvalid's rmse: 0.50172\n",
      "[253]\ttrain's rmse: 0.492842\tvalid's rmse: 0.501617\n",
      "[254]\ttrain's rmse: 0.492732\tvalid's rmse: 0.50157\n",
      "[255]\ttrain's rmse: 0.492593\tvalid's rmse: 0.501445\n",
      "[256]\ttrain's rmse: 0.49246\tvalid's rmse: 0.501337\n",
      "[257]\ttrain's rmse: 0.492347\tvalid's rmse: 0.501267\n",
      "[258]\ttrain's rmse: 0.492258\tvalid's rmse: 0.50118\n",
      "[259]\ttrain's rmse: 0.492161\tvalid's rmse: 0.501096\n",
      "[260]\ttrain's rmse: 0.491967\tvalid's rmse: 0.500901\n",
      "[261]\ttrain's rmse: 0.491884\tvalid's rmse: 0.500815\n",
      "[262]\ttrain's rmse: 0.491777\tvalid's rmse: 0.500706\n",
      "[263]\ttrain's rmse: 0.491647\tvalid's rmse: 0.500603\n",
      "[264]\ttrain's rmse: 0.491505\tvalid's rmse: 0.500449\n",
      "[265]\ttrain's rmse: 0.491426\tvalid's rmse: 0.500341\n",
      "[266]\ttrain's rmse: 0.491346\tvalid's rmse: 0.500307\n",
      "[267]\ttrain's rmse: 0.491225\tvalid's rmse: 0.500149\n",
      "[268]\ttrain's rmse: 0.491098\tvalid's rmse: 0.499999\n",
      "[269]\ttrain's rmse: 0.490957\tvalid's rmse: 0.499825\n",
      "[270]\ttrain's rmse: 0.490774\tvalid's rmse: 0.499676\n",
      "[271]\ttrain's rmse: 0.490666\tvalid's rmse: 0.499584\n",
      "[272]\ttrain's rmse: 0.490562\tvalid's rmse: 0.499515\n",
      "[273]\ttrain's rmse: 0.490479\tvalid's rmse: 0.499437\n",
      "[274]\ttrain's rmse: 0.490367\tvalid's rmse: 0.499377\n",
      "[275]\ttrain's rmse: 0.490247\tvalid's rmse: 0.499317\n",
      "[276]\ttrain's rmse: 0.490146\tvalid's rmse: 0.499242\n",
      "[277]\ttrain's rmse: 0.490031\tvalid's rmse: 0.499116\n",
      "[278]\ttrain's rmse: 0.489942\tvalid's rmse: 0.499038\n",
      "[279]\ttrain's rmse: 0.489847\tvalid's rmse: 0.498974\n",
      "[280]\ttrain's rmse: 0.489756\tvalid's rmse: 0.498927\n",
      "[281]\ttrain's rmse: 0.489625\tvalid's rmse: 0.498811\n",
      "[282]\ttrain's rmse: 0.489548\tvalid's rmse: 0.498772\n",
      "[283]\ttrain's rmse: 0.489446\tvalid's rmse: 0.498682\n",
      "[284]\ttrain's rmse: 0.489337\tvalid's rmse: 0.498564\n",
      "[285]\ttrain's rmse: 0.48922\tvalid's rmse: 0.498475\n",
      "[286]\ttrain's rmse: 0.489114\tvalid's rmse: 0.49839\n",
      "[287]\ttrain's rmse: 0.489018\tvalid's rmse: 0.498304\n",
      "[288]\ttrain's rmse: 0.488934\tvalid's rmse: 0.498259\n",
      "[289]\ttrain's rmse: 0.48882\tvalid's rmse: 0.498143\n",
      "[290]\ttrain's rmse: 0.488687\tvalid's rmse: 0.498018\n",
      "[291]\ttrain's rmse: 0.488584\tvalid's rmse: 0.497981\n",
      "[292]\ttrain's rmse: 0.488417\tvalid's rmse: 0.4979\n",
      "[293]\ttrain's rmse: 0.48834\tvalid's rmse: 0.497869\n",
      "[294]\ttrain's rmse: 0.488225\tvalid's rmse: 0.497721\n",
      "[295]\ttrain's rmse: 0.488154\tvalid's rmse: 0.497669\n",
      "[296]\ttrain's rmse: 0.488005\tvalid's rmse: 0.497555\n",
      "[297]\ttrain's rmse: 0.487873\tvalid's rmse: 0.497447\n",
      "[298]\ttrain's rmse: 0.487783\tvalid's rmse: 0.497354\n",
      "[299]\ttrain's rmse: 0.4877\tvalid's rmse: 0.497266\n",
      "[300]\ttrain's rmse: 0.487622\tvalid's rmse: 0.497204\n",
      "Time taken training LGBM  160.69816708564758.\n"
     ]
    }
   ],
   "source": [
    "#LGBM Training\n",
    "start = time.time()\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_train, y_train, \n",
    "                                                      test_size = 0.1, \n",
    "                                                      random_state = 144) \n",
    "evalset = [(train_X, train_y),(valid_X, valid_y)]\n",
    "#evalset = [(valid_X, valid_y)]\n",
    "lgbmodel = lgb.LGBMRegressor(learning_rate= 0.8,\n",
    "    objective='regression',\n",
    "    max_depth=4,\n",
    "    num_leaves=100,      #number of leaves in one tree\n",
    "    min_data_in_leaf=20, #minimal number of data in one leaf.\n",
    "    feature_fraction=1.0, #LightGBM will randomly select part of features on each iteration if feature_fraction smaller than 1.0.\n",
    "    min_split_gain=0.0, #the minimal gain to perform split\n",
    "    cat_l2=10.,         #L2 regularization in categorical split\n",
    "    min_data_in_bin=3,  #min number of data inside one bin, use this to avoid one-data-one-bin \n",
    "    silent=True,\n",
    "    metric='rmse',\n",
    "#    train_metric=False,\n",
    "#    metric_freq=10,\n",
    "    n_estimators=300,\n",
    "    cat_smooth=10, #this can reduce the effect of noises in categorical features, especially for categories with few data\n",
    "    max_bin=8192,\n",
    "    num_threads=2,\n",
    "    two_round_loading=True #set this to true if data file is too big to fit in memory\n",
    "        )\n",
    "lgbmodel.fit(X=train_X, y=train_y, eval_set=evalset, eval_names=['train', 'valid'],\n",
    "            eval_metric='rmse',\n",
    "            early_stopping_rounds=50 )\n",
    "\n",
    "\n",
    "preds = lgbmodel.predict(X_test)\n",
    "end = time.time()\n",
    "print(\"Time taken training LGBM  {}.\".format((end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ridge Training\n",
    "start = time.time()\n",
    "\n",
    "modelr = Ridge(solver = \"lsqr\", fit_intercept=False)\n",
    "modelr.fit(X_train, y_train)\n",
    "\n",
    "preds += modelr.predict(X_test)\n",
    "preds /= 2\n",
    "end = time.time()\n",
    "print(\"Time taken training Ridge  {}.\".format((end-start)))\n",
    "\n",
    "\n",
    "df_test[\"price\"] = np.expm1(preds)\n",
    "df_test[[\"test_id\", \"price\"]].to_csv(\"ff_LGBM_Ridge_3.csv\", index = False)\n",
    "end = time.time()\n",
    "print(\"Time taken by the Kernel  {}.\".format((end-fixstart)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LGBMRegressor' object has no attribute 'dump_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3e9ed70ba447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgbmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LGBMRegressor' object has no attribute 'dump_model'"
     ]
    }
   ],
   "source": [
    "lgbmodel.dump_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = lgbmodel.booster_\n",
    "booster.save_model('sk-lgbm-Ridge-03.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([144,  85,  30, ...,   0,   0,   0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=lgb.plot_importance(lgbmodel, max_num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
